{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Kaggle Competition: [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data Ingestion & Data Cleaning\n",
    "\n",
    "## 1.1 Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to read just 5 rows to check if \"data/train.csv\" was downloaded:\n",
    "try:\n",
    "    train_df = pd.read_csv('data/train.csv', nrows=5)   # total number of rows: 55423857\n",
    "    display(train_df.head())\n",
    "except FileNotFoundError:\n",
    "    print('>>> You must download train.csv from: https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/download/train.csv')\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tricks to load the data and decrease memory footprint significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column types to optimize memory usage\n",
    "col_types = {\n",
    "    'fare_amount': 'float32',\n",
    "    'pickup_datetime': 'str', \n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'passenger_count': 'uint8'\n",
    "}\n",
    "\n",
    "new_cols = list(col_types.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dataframe(read_size=0):   \n",
    "    # Method #1: read the entire DataFrame from the disk\n",
    "    if (read_size == 0):\n",
    "        df_list = [] # list to hold the batch dataframe\n",
    "        for df_chunk in tqdm(pd.read_csv('data/train.csv', usecols=new_cols, dtype=col_types, chunksize=5000000)): # loads 5M rows each iteration\n",
    "            df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n",
    "            df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "            # Process each chunk of dataframe right here:\n",
    "            # clean_data(), feature_engineer(),fit()\n",
    "\n",
    "            # Or, append the chunk to list and merge all later\n",
    "            df_list.append(df_chunk) \n",
    "\n",
    "        # merge all dataframes into one\n",
    "        train_df = pd.concat(df_list)\n",
    "        del df_list\n",
    "        return train_df\n",
    "        \n",
    "    # Method #2: read a predetermined amount of data         \n",
    "    train_df = pd.read_csv('data/train.csv', usecols=new_cols, dtype=col_types, nrows=read_size)\n",
    "    train_df['pickup_datetime'] = train_df['pickup_datetime'].str.slice(0, 16)\n",
    "    train_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_geo_outliers(df):\n",
    "    # NY boundary\n",
    "    boundary = { \n",
    "        'min_lng':-74.263242, 'min_lat':40.573143, \n",
    "        'max_lng':-72.986532, 'max_lat':41.709555 \n",
    "    }\n",
    "   \n",
    "    df.loc[ ((df.pickup_longitude  >= boundary['min_lng'] ) & \n",
    "             (df.pickup_longitude  <= boundary['max_lng']) &\n",
    "             (df.pickup_latitude   >= boundary['min_lat']) & \n",
    "             (df.pickup_latitude   <= boundary['max_lat']) &\n",
    "             (df.dropoff_longitude >= boundary['min_lng']) & \n",
    "             (df.dropoff_longitude <= boundary['max_lng']) &\n",
    "             (df.dropoff_latitude  >= boundary['min_lat']) & \n",
    "             (df.dropoff_latitude  <= boundary['max_lat'])),'outlier'] = 0\n",
    "    \n",
    "    df.loc[~((df.pickup_longitude  >= boundary['min_lng']) & \n",
    "             (df.pickup_longitude  <= boundary['max_lng']) &\n",
    "             (df.pickup_latitude   >= boundary['min_lat']) & \n",
    "             (df.pickup_latitude   <= boundary['max_lat']) &\n",
    "             (df.dropoff_longitude >= boundary['min_lng']) & \n",
    "             (df.dropoff_longitude <= boundary['max_lng']) &\n",
    "             (df.dropoff_latitude  >= boundary['min_lat']) & \n",
    "             (df.dropoff_latitude  <= boundary['max_lat'])),'outlier'] = 1    \n",
    "    \n",
    "#     print(\"Outlier vs Non Outlier\")\n",
    "#     print(df['is_outlier_loc'].value_counts())\n",
    "\n",
    "    df = df.loc[df['outlier'] == 0]    \n",
    "    return df.drop(['outlier'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_dataframe(num_rows):\n",
    "    train_df = get_raw_dataframe(num_rows)\n",
    "    \n",
    "    # remove NaNs and Negative & Free fares (keeping only the ones that cost more than 0)\n",
    "    train_df.dropna(inplace=True, axis=0)\n",
    "    train_df = train_df[train_df['fare_amount'] > 0]\n",
    "#     train_df = train_df[(train_df['fare_amount'] >= 2) & (train_df['fare_amount'] <= 200)]\n",
    "\n",
    "    # remove rows where lat/long are ZERO\n",
    "    train_df = train_df[(train_df['pickup_latitude'] != 0) & (train_df['pickup_longitude'] != 0)]\n",
    "    train_df = train_df[(train_df['dropoff_latitude'] != 0) & (train_df['dropoff_longitude'] != 0)]\n",
    "\n",
    "    # handle rides with too many or too few passengers\n",
    "    train_df = train_df[(train_df['passenger_count'] >= 0) & (train_df['passenger_count'] <= 7)]\n",
    "    \n",
    "    # remove any point beyond NYC border\n",
    "    train_df = remove_geo_outliers(train_df)\n",
    "        \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance formula: https://www.movable-type.co.uk/scripts/latlong.html\n",
    "def getDistance(lat1, lon1, lat2, lon2): \n",
    "    R = 6371e3\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    deltaPhi = math.radians(lat2-lat1)\n",
    "    deltaLambda = math.radians(lon2-lon1)\n",
    "\n",
    "    a = math.sin(deltaPhi/2) * math.sin(deltaPhi/2) + math.cos(phi1) * math.cos(phi2) * math.sin(deltaLambda/2) * math.sin(deltaLambda/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return round((R * c) / 1000, 3)    # returns distance in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calendar related features: year, month, day, hour, day_of_week\n",
    "def add_calendar(df):\n",
    "    df['year'] = df.pickup_datetime.apply(lambda x: x.year)\n",
    "    df['month'] = df.pickup_datetime.apply(lambda x: x.month)\n",
    "    df['day'] = df.pickup_datetime.apply(lambda x: x.day)\n",
    "    df['hour'] = df.pickup_datetime.apply(lambda x: x.hour)\n",
    "    df['day_of_week'] = df.pickup_datetime.apply(lambda x: x.weekday())\n",
    "    df['quarter'] = df.pickup_datetime.apply(lambda x: x.quarter)\n",
    "    df['week_of_year'] = df.pickup_datetime.apply(lambda x: x.isocalendar()[1])        \n",
    "    df['business_day'] = df.pickup_datetime.apply(lambda x: 1 if (x.weekday() >= 0 and x.weekday() <= 4) else 0)\n",
    "    df['daytime'] = 0\n",
    "    df.loc[(df.hour >= 8) & (df.hour < 20), 'daytime'] = 1 \n",
    "    \n",
    "    df['year'] = df['year'].astype('int16')\n",
    "    df['month'] = df['month'].astype('int8')\n",
    "    df['day'] = df['day'].astype('int8')\n",
    "    df['hour'] = df['hour'].astype('int8')\n",
    "    df['day_of_week'] = df['day_of_week'].astype('int8')    \n",
    "    df['quarter'] = df['quarter'].astype('int8')\n",
    "    df['week_of_year'] = df['week_of_year'].astype('int8')\n",
    "    df['daytime'] = df['daytime'].astype('int8')\n",
    "    df['business_day'] = df['business_day'].astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance in kilometers\n",
    "def add_distance_km(df):\n",
    "    df['distance_km'] = df.apply(lambda row: getDistance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    df['distance_km'] = df['distance_km'].astype('float32')\n",
    "\n",
    "#     df['distance_km_round'] = df.distance_km.apply(lambda x: int(round(x)))\n",
    "#     df['distance_km_round'] = df['distance_km_round'].astype('int16')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)\n",
    "\n",
    "def add_distances_mht_ecd(df):\n",
    "    df['distance_mht'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'], df['pickup_latitude'], df['dropoff_latitude'], 1)\n",
    "    df['distance_ecd'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'], df['pickup_latitude'], df['dropoff_latitude'], 2)    \n",
    "    \n",
    "    df['distance_mht'] = df['distance_mht'].astype('float32')\n",
    "    df['distance_ecd'] = df['distance_ecd'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_compass_bearing(lat1, long1, lat2, long2):\n",
    "    lat1 = math.radians(lat1)\n",
    "    lat2 = math.radians(lat2)\n",
    "\n",
    "    diffLong = math.radians(long2 - long1)\n",
    "\n",
    "    x = math.sin(diffLong) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)\n",
    "            * math.cos(lat2) * math.cos(diffLong))\n",
    "\n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # atan2() returns values from -180° to + 180°\n",
    "    # For a proper compass bearing, normalize the value as:\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360        \n",
    "    return round(compass_bearing, 3)\n",
    "\n",
    "\n",
    "def add_compass_bearing(df):\n",
    "    df['trip_direction'] = df.apply(lambda row: calc_compass_bearing(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    df['trip_direction'] = df['trip_direction'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical cluster as feature \n",
    "from sklearn.cluster import KMeans\n",
    "import copy\n",
    "\n",
    "# When this function is called with 1 arg, the arg is the training dataset. \n",
    "# When called with both args, the 1st arg is the test and the 2nd is the training dataset.\n",
    "def add_geo_cluster(df, train=pd.DataFrame()):\n",
    "    kmeans_geo_model = KMeans(n_clusters=4) # 4 clusters, as indicated by the Elbow method\n",
    "    \n",
    "    # The test and training datasets are passed so they can be clustered together and the test gets proper labels.\n",
    "    if (not train.empty):\n",
    "        df['is_test'] = 1\n",
    "        train['is_test'] = 0\n",
    "        geo_df = pd.concat([df,train], axis=0, sort=True)[['pickup_longitude','pickup_latitude','is_test']]\n",
    "    else:  \n",
    "        geo_df = copy.deepcopy(df[['pickup_longitude','pickup_latitude']])\n",
    "        \n",
    "    kmeans_geo_model.fit(geo_df[['pickup_longitude','pickup_latitude']])        \n",
    "    cluster_labels = kmeans_geo_model.predict(geo_df[['pickup_longitude','pickup_latitude']])    \n",
    "\n",
    "    # At this point, cluster_labels might have all the labels for the test & train datasets,\n",
    "    # and we can't simply do: df['geo_cluster'] = cluster_labels\n",
    "    # because when this function is called with both params, df will just store the test data.\n",
    "    # It's like trying to fit a big box into a smaller one.\n",
    "    geo_df['geo_cluster'] = cluster_labels    # SettingWithCopyWarning: geo_df cannot be just a slice of df\n",
    "    \n",
    "    if (train.empty):\n",
    "        df['geo_cluster'] = cluster_labels    \n",
    "        return df \n",
    "        \n",
    "    df['geo_cluster'] = geo_df[geo_df['is_test'] == 1].geo_cluster         \n",
    "    train.drop('is_test', axis=1, inplace=True)\n",
    "    df.drop('is_test', axis=1, inplace=True)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical difference (ride_start - ride_end) as feature \n",
    "def add_geo_diff(df):\n",
    "    df['lon_diff'] = df['dropoff_longitude'] - df['pickup_longitude']\n",
    "    df['lat_diff'] = df['dropoff_latitude']  - df['pickup_latitude']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical coordinates rounded for less precision\n",
    "def add_geo_coords_rounded(df):\n",
    "    df['pickup_latitude_round']    = df['pickup_latitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['pickup_longitude_round']   = df['pickup_longitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['dropoff_latitude_round']   = df['dropoff_latitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['dropoff_longitude_round']  = df['dropoff_longitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features that define if a trip was to a particular airport OR neighbourhood\n",
    "def setLocationSrcDst(df, boundary, srcName, dstName):\n",
    "    df[srcName] = 0             \n",
    "    df.loc[((df.pickup_latitude  >= boundary['min_lat']) & (df.pickup_latitude  <= boundary['max_lat']) &\n",
    "            (df.pickup_longitude >= boundary['min_lng']) & (df.pickup_longitude <= boundary['max_lng'])), srcName] = 1    \n",
    "    \n",
    "    df[dstName] = 0\n",
    "    df.loc[((df.dropoff_latitude  >= boundary['min_lat']) & (df.dropoff_latitude  <= boundary['max_lat']) &\n",
    "            (df.dropoff_longitude >= boundary['min_lng']) & (df.dropoff_longitude <= boundary['max_lng'])), dstName] = 1\n",
    "    \n",
    "    df[srcName] = df[srcName].astype('int8')\n",
    "    df[dstName] = df[dstName].astype('int8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_airports(df):\n",
    "    JFK = { 'min_lng': -73.8352, 'min_lat': 40.6195, 'max_lng': -73.7401, 'max_lat': 40.6659 }        \n",
    "    EWR = { 'min_lng': -74.1925, 'min_lat': 40.6700, 'max_lng': -74.1531, 'max_lat': 40.7081 }\n",
    "    LGA = { 'min_lng': -73.8895, 'min_lat': 40.7664, 'max_lng': -73.8550, 'max_lat': 40.7931 }\n",
    "    \n",
    "    df = setLocationSrcDst(df, JFK, 'jfk_src', 'jfk_dst')        \n",
    "    df = setLocationSrcDst(df, EWR, 'ewr_src', 'ewr_dst')        \n",
    "    df = setLocationSrcDst(df, LGA, 'lga_src', 'lga_dst')    \n",
    "    return df   \n",
    "\n",
    "\n",
    "def add_locations(df):\n",
    "    manhattan =     { 'min_lng': -74.0479, 'min_lat': 40.6829, 'max_lng': -73.9067, 'max_lat': 40.8820 }                \n",
    "    queens =        { 'min_lng': -73.9630, 'min_lat': 40.5431, 'max_lng': -73.7004, 'max_lat': 40.8007 }\n",
    "    brooklyn =      { 'min_lng': -74.0421, 'min_lat': 40.5707, 'max_lng': -73.8334, 'max_lat': 40.7395 }\n",
    "    bronx =         { 'min_lng': -73.9339, 'min_lat': 40.7855, 'max_lng':-73.7654,  'max_lat': 40.9176 }\n",
    "    staten_island = { 'min_lng': -74.2558, 'min_lat': 40.4960, 'max_lng': -74.0522, 'max_lat': 40.6490 }\n",
    "    \n",
    "    df = setLocationSrcDst(df, manhattan, 'man_src', 'man_dst')\n",
    "    df = setLocationSrcDst(df, queens, 'qns_src', 'qns_dst')\n",
    "    df = setLocationSrcDst(df, brooklyn, 'bkny_src', 'bkny_dst')\n",
    "    df = setLocationSrcDst(df, bronx, 'bx_src', 'bx_dst')\n",
    "    df = setLocationSrcDst(df, staten_island, 'si_src', 'si_dst')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour_statistics(df):   \n",
    "    # peak hour only exists on business days\n",
    "    df['peak_hour'] = np.where((df['hour'] >= 16) & (df['hour'] <= 20) & (df['business_day'] == 1), 1, 0)\n",
    "        \n",
    "    # there is a daily 50-cent surcharge from 8pm to 6am\n",
    "    df['night_hour'] = np.where((df['hour'] >= 20) | (df['hour'] <= 6) , 1, 0)\n",
    "    \n",
    "    df['peak_hour'] = df['peak_hour'].astype('int8')\n",
    "    df['night_hour'] = df['night_hour'].astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ratio for (rides_per_hour and rides_per_day) (only works for the training set)\n",
    "def add_ride_statistics(df):    \n",
    "    tmp_hour = df.groupby(['year','month','day','hour']).agg({'fare_amount' : [np.size]}).reset_index()\n",
    "    tmp_hour.columns = ['year','month','day','hour','rides_per_hour']\n",
    "\n",
    "    tmp_day = df.groupby(['year','month','day']).agg({'fare_amount' : [np.size]}).reset_index()\n",
    "    tmp_day.columns = ['year','month','day','rides_per_day']\n",
    "            \n",
    "    df = pd.merge(df, tmp_hour, how='left', on=['year','month','day','hour'])\n",
    "    df = pd.merge(df, tmp_day, how='left', on=['year','month','day'])\n",
    "        \n",
    "    df['rides_per_hour'] = df['rides_per_hour'].astype('int16')\n",
    "    df['rides_per_day'] = df['rides_per_day'].astype('int16')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean/median/min/max fare (only works well for the training/test set, not for the Kaggle scoring system)\n",
    "def add_fare_statistics(df):\n",
    "    tmp = df.groupby(['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'fare_amount' : [np.mean,np.median,np.min,np.max]}).reset_index()\n",
    "    tmp.columns = ['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "    df = pd.merge(df, tmp, how='left', on=['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cost per KM (only works well for the training/test set, not for the Kaggle scoring system)\n",
    "def add_cost_per_km(df):\n",
    "    df['cost_per_km'] = df.apply(lambda row: row['distance_km_round'] / row['fare_amount'], axis=1)    \n",
    "    df['cost_per_km'] = df['cost_per_km'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Model Training\n",
    "\n",
    "## 3.1 Feature Selection\n",
    "\n",
    "Start by specifying which features should be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the use of the following features:\n",
    "use_feature_calendar           = True\n",
    "use_feature_distance_km        = True\n",
    "use_feature_distances          = True\n",
    "use_feature_trip_direction     = True\n",
    "use_feature_geo_coords_rounded = False\n",
    "use_feature_geo_cluster        = True\n",
    "use_feature_geo_diff           = True\n",
    "use_feature_airports           = True\n",
    "use_feature_locations          = True\n",
    "use_feature_hour_statistics    = True\n",
    "\n",
    "# Bad features, don't use these:\n",
    "use_feature_ride_statistics    = False\n",
    "use_feature_fare_statistics    = False\n",
    "use_feature_cost_per_km        = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Retrive a clean dataframe from the disk with N rows and add features to it\n",
    "train_df = get_clean_dataframe(5*1000000) # loads (N * 1M) rows\n",
    "\n",
    "if (use_feature_calendar):\n",
    "    train_df = add_calendar(train_df)\n",
    "    \n",
    "if (use_feature_hour_statistics):\n",
    "    train_df = add_hour_statistics(train_df)    \n",
    "\n",
    "if (use_feature_distance_km):    \n",
    "    train_df = add_distance_km(train_df)\n",
    "    \n",
    "if (use_feature_distances):    \n",
    "    train_df = add_distances_mht_ecd(train_df)\n",
    "\n",
    "if (use_feature_trip_direction):\n",
    "    train_df = add_compass_bearing(train_df)    \n",
    "    \n",
    "if (use_feature_geo_coords_rounded):\n",
    "    train_df = add_geo_coords_rounded(train_df)\n",
    "\n",
    "if (use_feature_geo_cluster):    \n",
    "    train_df = add_geo_cluster(train_df)\n",
    "\n",
    "if (use_feature_geo_diff):    \n",
    "    train_df = add_geo_diff(train_df)\n",
    "\n",
    "if (use_feature_airports):\n",
    "    train_df = add_airports(train_df)\n",
    "\n",
    "if (use_feature_locations):    \n",
    "    train_df = add_locations(train_df)\n",
    "    \n",
    "if (use_feature_ride_statistics):\n",
    "    train_df = add_ride_statistics(train_df)\n",
    "\n",
    "if (use_feature_fare_statistics):\n",
    "    train_df = add_fare_statistics(train_df)\n",
    "\n",
    "if (use_feature_cost_per_km):    \n",
    "    train_df = add_cost_per_km(train_df)\n",
    "    \n",
    "display(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# print_mem_usage(): outputs the memory usage from df.info(memory_usage='deep')\n",
    "def print_mem_usage(df):    \n",
    "    buf = io.StringIO()\n",
    "    df.info(buf=buf, memory_usage='deep')\n",
    "    mem_usage = buf.getvalue().split('\\n')[-2]\n",
    "    print('>>> Memory usage:', mem_usage)\n",
    "\n",
    "    \n",
    "# print column names\n",
    "# print(train_df.columns)\n",
    "    \n",
    "# display feature related statistics    \n",
    "pd.set_option('display.max_columns', 200)       # tells pandas to print all columns (no hiding!)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('float_format', '{:f}'.format)    # prints the entire number instead of x + ye\n",
    "display(train_df.describe())\n",
    "\n",
    "print_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the correlation between the features\n",
    "# train_subset = train_df[:6000]\n",
    "# sns.pairplot(train_subset, vars=['fare_amount','distance_km_round','lon_diff','lat_diff','geo_cluster','year','month','hour'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the correlation of 'fare_amount' with all the other features\n",
    "print(train_df.corrwith(train_df['fare_amount']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete features with corr less than 0.1\n",
    "bad_features_list = ['passenger_count','hour','day','month','day_of_week']\n",
    "train_df.drop(bad_features_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression, RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import compress\n",
    "\n",
    "target = train_df[['fare_amount']]\n",
    "df = train_df.drop(['fare_amount', 'pickup_datetime'], axis=1)\n",
    "\n",
    "X = df\n",
    "y = target.values.ravel()\n",
    "\n",
    "# Split X,y into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('>>> Train/Test Split done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hyperparameter Tuning (*optional*)\n",
    "\n",
    "Change `EXEC_GRID_SEARCH` to **True** and be able to run `RandomizedSearchCV()` to find optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import randint as sp_randint\n",
    "# g = sp_randint(1, 11)\n",
    "# print(g.rvs(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "def mean_squared_error_(ground_truth, predictions):\n",
    "    return math.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "\n",
    "RMSE = make_scorer(mean_squared_error_, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "EXEC_GRID_SEARCH = False\n",
    "print('>>> Perform grid search:', EXEC_GRID_SEARCH)\n",
    "\n",
    "if (not EXEC_GRID_SEARCH):\n",
    "#     rfr_model = RandomForestRegressor(n_jobs=7, n_estimators=25, max_features=len(df.columns), max_depth=25, min_samples_split=3, min_samples_leaf=3, random_state=24)\n",
    "    rfr_model = RandomForestRegressor(n_jobs=-1, n_estimators=35, max_features=len(df.columns), max_depth=28, min_samples_split=4, min_samples_leaf=4, random_state=24)\n",
    "else:    \n",
    "    param_grid = {\n",
    "        'n_estimators'     : sp_randint(8, 45),\n",
    "        'max_depth'        : sp_randint(3, 20),\n",
    "        'max_features'     : sp_randint(1, len(df.columns)),                \n",
    "        'min_samples_split': sp_randint(2, 11),\n",
    "        'min_samples_leaf' : sp_randint(1, 11),\n",
    "        'bootstrap'        : [True, False]\n",
    "    }\n",
    "\n",
    "    # Estimator for use in random search\n",
    "    estimator = RandomForestRegressor(random_state=24)\n",
    "\n",
    "    # Create the random search model\n",
    "    n_iter_search = 500\n",
    "    random_search = RandomizedSearchCV(n_jobs=6, estimator=estimator, param_distributions=param_grid, scoring=RMSE, cv=3, n_iter=n_iter_search, verbose=50, random_state=24)\n",
    "    random_search.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "    rfr_model = random_search.best_estimator_\n",
    "    print('>>> Best Score (RMSE):', random_search.best_score_)\n",
    "    print(f'>>> Best parameters: {random_search.best_params_}')\n",
    "        \n",
    "#     report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the complete results\n",
    "if (EXEC_GRID_SEARCH):\n",
    "    print(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the model using RandomForestRegressor\n",
    "rfr_model.fit(X_train, np.log1p(y_train))\n",
    "print('>>> Model Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation\n",
    "\n",
    "Evaluates the model on the holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import gfx  # gfx is another local module under the 'lib' directory\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "# The prediction is calculated as the arithmetic mean of both model's predictions\n",
    "# gbr_fare = np.expm1(gbr_model.predict(X_test))  # model 1\n",
    "rfr_fare = np.expm1(rfr_model.predict(X_test))   # model 2\n",
    "\n",
    "# y_pred = (gbr_visitors + rfr_visitors) / 2\n",
    "# y_pred = gbr_fare\n",
    "y_pred = rfr_fare\n",
    "\n",
    "rmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('\\n>>> RMSE Score:', rmse_score, '\\n') # 2.5M -> 3.6863\n",
    "\n",
    "# Plot Actual fare data against Predicted fare\n",
    "gfx.plot_actual_vs_predicted(y, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[:df.shape[1]]\n",
    "importances = rfr_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission\n",
    "\n",
    "## 5.1 Feature Engineering for the Submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ride-based features for the test set, copying from the training set\n",
    "def test_add_ride_statistics(test, train):\n",
    "    short_train_df = train[['year','month','day','hour','rides_per_hour']]\n",
    "    tmp_hour = short_train_df.groupby(['year','month','day','hour']).agg({'rides_per_hour' : [np.mean]}).reset_index()\n",
    "    tmp_hour.columns = ['year','month','day','hour','rides_per_hour']\n",
    "    test = pd.merge(test, tmp_hour, how='left', on=['year','month','day','hour']) \n",
    "\n",
    "    short_train_df = train[['year','month','day','rides_per_day']]\n",
    "    tmp_day = short_train_df.groupby(['year','month','day']).agg({'rides_per_day' : [np.mean]}).reset_index()\n",
    "    tmp_day.columns = ['year','month','day','rides_per_day']\n",
    "    test = pd.merge(test, tmp_day, how='left', on=['year','month','day'])\n",
    "    \n",
    "    # At this point, there might be some NaNs laying around    \n",
    "    test[\"rides_per_day\"].fillna(method='ffill', inplace=True) \n",
    "    test[\"rides_per_hour\"].fillna(method='ffill', inplace=True) \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean/median/min/max fare statistics for the test set\n",
    "def test_add_statistics(test, train):\n",
    "    # Score 7.82    \n",
    "#     short_train_df = train[['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "#     tmp = short_train_df.groupby(['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "#     tmp.columns = ['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']    \n",
    "#     test = pd.merge(test, tmp, how='left', on=['year','month','day_of_week','hour','distance_km_round','taxis_per_hour'])          \n",
    "\n",
    "    # Score 6.59\n",
    "    short_train_df = train[['hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "    tmp = short_train_df.groupby(['hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "    tmp.columns = ['hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "    test = pd.merge(test, tmp, how='left', on=['hour','distance_km_round','taxis_per_hour'])  \n",
    "\n",
    "    # Score 7.58\n",
    "#     short_train_df = train[['day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "#     tmp = short_train_df.groupby(['day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "#     tmp.columns = ['day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "#     test = pd.merge(test, tmp, how='left', on=['day_of_week','hour','distance_km_round','taxis_per_hour'])  \n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_cost_per_km(test):    \n",
    "    test['cost_per_km'] = test.apply(lambda row: row['distance_km_round'] / row['mean_fare'], axis=1)\n",
    "    test['cost_per_km'] = test['cost_per_km'].astype('float32')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Feature Selection for the Submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the test dataset used for submission\n",
    "col_types = {\n",
    "    'key': 'str',\n",
    "    'pickup_datetime': 'str', \n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'passenger_count': 'uint8'\n",
    "}\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv', usecols=list(col_types.keys()), dtype=col_types)\n",
    "test_df['pickup_datetime'] = test_df['pickup_datetime'].str.slice(0, 16)\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "display(test_df.head(3))\n",
    "print('>>> test_df shape:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add common features\n",
    "if (use_feature_calendar):\n",
    "    test_df = add_calendar(test_df)\n",
    "        \n",
    "if (use_feature_hour_statistics):\n",
    "    test_df = add_hour_statistics(test_df)            \n",
    "        \n",
    "if (use_feature_distance_km):    \n",
    "    test_df = add_distance_km(test_df)\n",
    "\n",
    "if (use_feature_distances):    \n",
    "    test_df = add_distances_mht_ecd(test_df)\n",
    "\n",
    "if (use_feature_trip_direction):\n",
    "    test_df = add_compass_bearing(test_df)        \n",
    "    \n",
    "if (use_feature_geo_coords_rounded):    \n",
    "    test_df = add_geo_coords_rounded(test_df)\n",
    "\n",
    "if (use_feature_geo_cluster):    \n",
    "    test_df = add_geo_cluster(test_df, train_df)\n",
    "\n",
    "if (use_feature_geo_diff):    \n",
    "    test_df = add_geo_diff(test_df)\n",
    "\n",
    "if (use_feature_airports):    \n",
    "    test_df = add_airports(test_df)\n",
    "\n",
    "if (use_feature_locations):    \n",
    "    test_df = add_locations(test_df)\n",
    "    \n",
    "if (use_feature_ride_statistics):    \n",
    "    test_df = test_add_ride_statistics(test_df, train_df)\n",
    "\n",
    "if (use_feature_fare_statistics): \n",
    "    test_df = add_fare_statistics(test_df, train_df)        \n",
    "    # print(test_df.isnull().sum())               # check missing data\n",
    "    # test_df = test_df.fillna(test_df.mean())    # handle missing data    \n",
    "        \n",
    "if (use_feature_cost_per_km): \n",
    "    test_df = test_add_cost_per_km(test_df, train_df)         \n",
    "    \n",
    "# Remove weak features\n",
    "test_df.drop(bad_features_list, axis=1, inplace=True)    \n",
    "\n",
    "display(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: NaNs\n",
    "# print(test_df.isnull().sum())                             # check missing data\n",
    "# display(test_df[test_df.isnull().any(axis=1)].head())     # print NaN rows      \n",
    "# display(test_df.iloc[10:13])                              # print specific rows\n",
    "\n",
    "display(test_df.describe())\n",
    "# print(test_df.info())\n",
    "\n",
    "print_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "L1 = train_df.columns.tolist()\n",
    "L2 = test_df.columns.tolist()\n",
    "print('>>> Only *fare_amount* can show up as the difference between both datasets.\\n>>> Found:', list(set(L1) - set(L2)))\n",
    "print('>>> Number of Features for training=', len(L1), 'and testing=', len(L2))\n",
    "\n",
    "tc = unittest.TestCase('__init__')\n",
    "tc.assertEqual(len(L1) == len(L2), True)  # test_df includes 'key', so this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Train & run the final Model on the Submission dataset\n",
    "\n",
    "This step stores the final results in *submission.csv*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Select the submission data to be used on the final model\n",
    "X = train_df.drop(['fare_amount', 'pickup_datetime'], axis=1)\n",
    "y = train_df[['fare_amount']].values.ravel()\n",
    "\n",
    "### Create and fit the model\n",
    "# rfr_model = RandomForestRegressor(n_jobs=7, n_estimators=25, max_features=len(df.columns), max_depth=25, min_samples_split=3, min_samples_leaf=3, random_state=42)\n",
    "rfr_model = RandomForestRegressor(n_jobs=7, n_estimators=35, max_features=1.0, max_depth=28, min_samples_split=4, min_samples_leaf=4, random_state=42)\n",
    "rfr_model.fit(X, np.log1p(y))\n",
    "\n",
    "### Predict fare\n",
    "X_test = test_df.drop(['key', 'pickup_datetime'], axis=1)\n",
    "y_pred = np.expm1(rfr_model.predict(X_test))\n",
    "\n",
    "\n",
    "### Save submission file\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "submission_df['fare_amount'] = y_pred\n",
    "submission_df.to_csv('submission_xgb.csv', index=False)\n",
    "\n",
    "display(submission_df.head())\n",
    "print('>>> submission_df shape=', submission_df.shape)\n",
    "print('>>> File saved sucessfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print total notebook runtime for debugging purposes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "elapsed_time = (time.time() - start_time)\n",
    "print('>>> Runtime:', str(timedelta(seconds=elapsed_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
