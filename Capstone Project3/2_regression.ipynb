{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Kaggle Competition: [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Ingestion & Data Cleaning\n",
    "\n",
    "## 1.1 Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rows = 55423857\n",
    "# print (f'>>> Exact number of rows: {train_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data/train.csv exists:\n",
    "try:\n",
    "    train_df = pd.read_csv('data/train.csv', nrows=5)\n",
    "except FileNotFoundError:\n",
    "    print('>>> You must download train.csv from: https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/download/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tricks to load the data and decrease memory footprint significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column types to optimize memory usage\n",
    "col_types = {\n",
    "    'fare_amount': 'float32',\n",
    "    'pickup_datetime': 'str', \n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'passenger_count': 'uint8'\n",
    "}\n",
    "\n",
    "new_cols = list(col_types.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dataframe(read_size=0):   \n",
    "    # Method #1: read the entire DataFrame from the disk\n",
    "    if (read_size == 0):\n",
    "        df_list = [] # list to hold the batch dataframe\n",
    "        for df_chunk in tqdm(pd.read_csv('data/train.csv', usecols=new_cols, dtype=col_types, chunksize=5000000)): # loads 5M rows each iteration\n",
    "            df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n",
    "            df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "            # Process each chunk of dataframe right here:\n",
    "            # clean_data(), feature_engineer(),fit()\n",
    "\n",
    "            # Or, append the chunk to list and merge all later\n",
    "            df_list.append(df_chunk) \n",
    "\n",
    "        # merge all dataframes into one\n",
    "        train_df = pd.concat(df_list)\n",
    "        del df_list\n",
    "        return train_df\n",
    "        \n",
    "    # Method #2: read a predetermined amount of data         \n",
    "    train_df = pd.read_csv('data/train.csv', usecols=new_cols, dtype=col_types, nrows=read_size)\n",
    "    train_df['pickup_datetime'] = train_df['pickup_datetime'].str.slice(0, 16)\n",
    "    train_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.020000</td>\n",
       "      <td>-73.959664</td>\n",
       "      <td>40.739009</td>\n",
       "      <td>-73.952068</td>\n",
       "      <td>40.757340</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.102156</td>\n",
       "      <td>0.066789</td>\n",
       "      <td>0.024756</td>\n",
       "      <td>0.063357</td>\n",
       "      <td>0.029084</td>\n",
       "      <td>0.447214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.300000</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.700000</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.900000</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count\n",
       "count     5.000000          5.000000         5.000000           5.000000          5.000000         5.000000\n",
       "mean      8.020000        -73.959664        40.739009         -73.952068         40.757340         1.200000\n",
       "std       5.102156          0.066789         0.024756           0.063357          0.029084         0.447214\n",
       "min       4.500000        -74.016048        40.711303         -73.991567         40.712278         1.000000\n",
       "25%       5.300000        -73.987130        40.721319         -73.991242         40.750562         1.000000\n",
       "50%       5.700000        -73.982738        40.733143         -73.979268         40.758092         1.000000\n",
       "75%       7.700000        -73.968095        40.761270         -73.956655         40.782004         1.000000\n",
       "max      16.900000        -73.844311        40.768008         -73.841610         40.783762         2.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check feature related statistics\n",
    "pd.set_option('display.max_columns', 100)       # tells pandas to print all columns (no hiding!)\n",
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('float_format', '{:f}'.format)    # prints the entire number instead of x + ye\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_geo_outliers(df):\n",
    "    # NY boundary\n",
    "    boundary = { \n",
    "        'min_lng':-74.263242, 'min_lat':40.573143, \n",
    "        'max_lng':-72.986532, 'max_lat':41.709555 \n",
    "    }\n",
    "   \n",
    "    df.loc[ ((df.pickup_longitude  >= boundary['min_lng'] ) & \n",
    "             (df.pickup_longitude  <= boundary['max_lng']) &\n",
    "             (df.pickup_latitude   >= boundary['min_lat']) & \n",
    "             (df.pickup_latitude   <= boundary['max_lat']) &\n",
    "             (df.dropoff_longitude >= boundary['min_lng']) & \n",
    "             (df.dropoff_longitude <= boundary['max_lng']) &\n",
    "             (df.dropoff_latitude  >= boundary['min_lat']) & \n",
    "             (df.dropoff_latitude  <= boundary['max_lat'])),'outlier'] = 0\n",
    "    \n",
    "    df.loc[~((df.pickup_longitude  >= boundary['min_lng']) & \n",
    "             (df.pickup_longitude  <= boundary['max_lng']) &\n",
    "             (df.pickup_latitude   >= boundary['min_lat']) & \n",
    "             (df.pickup_latitude   <= boundary['max_lat']) &\n",
    "             (df.dropoff_longitude >= boundary['min_lng']) & \n",
    "             (df.dropoff_longitude <= boundary['max_lng']) &\n",
    "             (df.dropoff_latitude  >= boundary['min_lat']) & \n",
    "             (df.dropoff_latitude  <= boundary['max_lat'])),'outlier'] = 1    \n",
    "    \n",
    "#     print(\"Outlier vs Non Outlier\")\n",
    "#     print(df['is_outlier_loc'].value_counts())\n",
    "\n",
    "    df = df.loc[df['outlier'] == 0]    \n",
    "    return df.drop(['outlier'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_dataframe(num_rows):\n",
    "    train_df = get_raw_dataframe(num_rows)\n",
    "    \n",
    "    # remove NaNs and Negative & Free fares (keeping only the ones that cost more than 0)\n",
    "    train_df.dropna(inplace=True, axis=0)\n",
    "    train_df = train_df[train_df['fare_amount'] > 0]\n",
    "#     train_df = train_df[(train_df['fare_amount'] >= 2) & (train_df['fare_amount'] <= 200)]\n",
    "\n",
    "    # remove rows where lat/long are ZERO\n",
    "    train_df = train_df[(train_df['pickup_latitude'] != 0) & (train_df['pickup_longitude'] != 0)]\n",
    "    train_df = train_df[(train_df['dropoff_latitude'] != 0) & (train_df['dropoff_longitude'] != 0)]\n",
    "\n",
    "    # handle rides with too many or too few passengers\n",
    "    train_df = train_df[(train_df['passenger_count'] >= 0) & (train_df['passenger_count'] <= 7)]\n",
    "    \n",
    "    # remove any point beyond NYC border\n",
    "    train_df = remove_geo_outliers(train_df)\n",
    "    \n",
    "    # remove useless feature: passenger_count\n",
    "    train_df.drop('passenger_count', axis=1, inplace=True)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance formula: https://www.movable-type.co.uk/scripts/latlong.html\n",
    "def getDistance(lat1, lon1, lat2, lon2): \n",
    "    R = 6371e3\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    deltaPhi = math.radians(lat2-lat1)\n",
    "    deltaLambda = math.radians(lon2-lon1)\n",
    "\n",
    "    a = math.sin(deltaPhi/2) * math.sin(deltaPhi/2) + math.cos(phi1) * math.cos(phi2) * math.sin(deltaLambda/2) * math.sin(deltaLambda/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return round((R * c) / 1000, 3)    # returns distance in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calendar related features: year, month, day, hour, day_of_week\n",
    "def add_calendar(df):\n",
    "    df['year'] = df.pickup_datetime.apply(lambda x: x.year)\n",
    "    df['month'] = df.pickup_datetime.apply(lambda x: x.month)\n",
    "    df['day'] = df.pickup_datetime.apply(lambda x: x.day)\n",
    "    df['hour'] = df.pickup_datetime.apply(lambda x: x.hour)\n",
    "    df['day_of_week'] = df.pickup_datetime.apply(lambda x: x.weekday())\n",
    "    df['quarter'] = df.pickup_datetime.apply(lambda x: x.quarter)\n",
    "    df['week_of_year'] = df.pickup_datetime.apply(lambda x: x.isocalendar()[1])        \n",
    "    df['business_day'] = df.pickup_datetime.apply(lambda x: 1 if (x.weekday() >= 0 and x.weekday() <= 4) else 0)\n",
    "    df['daytime'] = 0\n",
    "    df.loc[(df.hour >= 8) & (df.hour < 20), 'daytime'] = 1 \n",
    "    \n",
    "    df['year'] = df['year'].astype('int16')\n",
    "    df['month'] = df['month'].astype('int8')\n",
    "    df['day'] = df['day'].astype('int8')\n",
    "    df['hour'] = df['hour'].astype('int8')\n",
    "    df['day_of_week'] = df['day_of_week'].astype('int8')    \n",
    "    df['quarter'] = df['quarter'].astype('int8')\n",
    "    df['week_of_year'] = df['week_of_year'].astype('int8')\n",
    "    df['daytime'] = df['daytime'].astype('int8')\n",
    "    df['business_day'] = df['business_day'].astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance in kilometers\n",
    "def add_distance_km(df):\n",
    "    df['distance_km'] = df.apply(lambda row: getDistance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    df['distance_km'] = df['distance_km'].astype('float32')\n",
    "\n",
    "    df['distance_km_round'] = df.distance_km.apply(lambda x: int(round(x)))\n",
    "    df['distance_km_round'] = df['distance_km_round'].astype('int16')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)\n",
    "\n",
    "def add_distances_mht_ecd(df):\n",
    "    df['distance_mht'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'], df['pickup_latitude'], df['dropoff_latitude'], 1)\n",
    "    df['distance_ecd'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'], df['pickup_latitude'], df['dropoff_latitude'], 2)    \n",
    "    \n",
    "    df['distance_mht'] = df['distance_mht'].astype('float32')\n",
    "    df['distance_ecd'] = df['distance_ecd'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical cluster as feature \n",
    "from sklearn.cluster import KMeans\n",
    "import copy\n",
    "\n",
    "# When this function is called with 1 arg, the arg is the training dataset. \n",
    "# When called with both args, the 1st arg is the test and the 2nd is the training dataset.\n",
    "def add_geo_cluster(df, train=pd.DataFrame()):\n",
    "    kmeans_geo_model = KMeans(n_clusters=4) # 4 clusters, as indicated by the Elbow method\n",
    "    \n",
    "    # The test and training datasets are passed so they can be clustered together and the test gets proper labels.\n",
    "    if (not train.empty):\n",
    "        df['is_test'] = 1\n",
    "        train['is_test'] = 0\n",
    "        geo_df = pd.concat([df,train], axis=0, sort=True)[['pickup_longitude','pickup_latitude','is_test']]\n",
    "    else:  \n",
    "        geo_df = copy.deepcopy(df[['pickup_longitude','pickup_latitude']])\n",
    "        \n",
    "    kmeans_geo_model.fit(geo_df[['pickup_longitude','pickup_latitude']])        \n",
    "    cluster_labels = kmeans_geo_model.predict(geo_df[['pickup_longitude','pickup_latitude']])    \n",
    "\n",
    "    # At this point, cluster_labels might have all the labels for the test & train datasets,\n",
    "    # and we can't simply do: df['geo_cluster'] = cluster_labels\n",
    "    # because when this function is called with both params, df will just store the test data.\n",
    "    # It's like trying to fit a big box into a smaller one.\n",
    "    geo_df['geo_cluster'] = cluster_labels    # SettingWithCopyWarning: geo_df cannot be just a slice of df\n",
    "    \n",
    "    if (train.empty):\n",
    "        df['geo_cluster'] = cluster_labels    \n",
    "        return df \n",
    "        \n",
    "    df['geo_cluster'] = geo_df[geo_df['is_test'] == 1].geo_cluster         \n",
    "    train.drop('is_test', axis=1, inplace=True)\n",
    "    df.drop('is_test', axis=1, inplace=True)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical difference (ride_start - ride_end) as feature \n",
    "def add_geo_diff(df):\n",
    "    df['lon_diff'] = df['dropoff_longitude'] - df['pickup_longitude']\n",
    "    df['lat_diff'] = df['dropoff_latitude']  - df['pickup_latitude']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add geographical coordinates rounded for less precision\n",
    "def add_geo_coords_rounded(df):\n",
    "    df['pickup_latitude_round']    = df['pickup_latitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['pickup_longitude_round']   = df['pickup_longitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['dropoff_latitude_round']   = df['dropoff_latitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    df['dropoff_longitude_round']  = df['dropoff_longitude'].apply(lambda x:round(x,3)).astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features that define if a trip was to a particular airport OR neighbourhood\n",
    "def setLocationSrcDst(df, boundary, srcName, dstName):\n",
    "    df[srcName] = 0             \n",
    "    df.loc[((df.pickup_latitude  >= boundary['min_lat']) & (df.pickup_latitude  <= boundary['max_lat']) &\n",
    "            (df.pickup_longitude >= boundary['min_lng']) & (df.pickup_longitude <= boundary['max_lng'])), srcName] = 1    \n",
    "    \n",
    "    df[dstName] = 0\n",
    "    df.loc[((df.dropoff_latitude  >= boundary['min_lat']) & (df.dropoff_latitude  <= boundary['max_lat']) &\n",
    "            (df.dropoff_longitude >= boundary['min_lng']) & (df.dropoff_longitude <= boundary['max_lng'])), dstName] = 1\n",
    "    \n",
    "    df[srcName] = df[srcName].astype('int8')\n",
    "    df[dstName] = df[dstName].astype('int8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_airports(df):\n",
    "    JFK = { 'min_lng': -73.8352, 'min_lat': 40.6195, 'max_lng': -73.7401, 'max_lat': 40.6659 }        \n",
    "    EWR = { 'min_lng': -74.1925, 'min_lat': 40.6700, 'max_lng': -74.1531, 'max_lat': 40.7081 }\n",
    "    LGA = { 'min_lng': -73.8895, 'min_lat': 40.7664, 'max_lng': -73.8550, 'max_lat': 40.7931 }\n",
    "    \n",
    "    df = setLocationSrcDst(df, JFK, 'jfk_src', 'jfk_dst')        \n",
    "    df = setLocationSrcDst(df, EWR, 'ewr_src', 'ewr_dst')        \n",
    "    df = setLocationSrcDst(df, LGA, 'lga_src', 'lga_dst')    \n",
    "    return df   \n",
    "\n",
    "\n",
    "def add_locations(df):\n",
    "    manhattan =     { 'min_lng': -74.0479, 'min_lat': 40.6829, 'max_lng': -73.9067, 'max_lat': 40.8820 }                \n",
    "    queens =        { 'min_lng': -73.9630, 'min_lat': 40.5431, 'max_lng': -73.7004, 'max_lat': 40.8007 }\n",
    "    brooklyn =      { 'min_lng': -74.0421, 'min_lat': 40.5707, 'max_lng': -73.8334, 'max_lat': 40.7395 }\n",
    "    bronx =         { 'min_lng': -73.9339, 'min_lat': 40.7855, 'max_lng':-73.7654,  'max_lat': 40.9176 }\n",
    "    staten_island = { 'min_lng': -74.2558, 'min_lat': 40.4960, 'max_lng': -74.0522, 'max_lat': 40.6490 }\n",
    "    \n",
    "    df = setLocationSrcDst(df, manhattan, 'man_src', 'man_dst')\n",
    "    df = setLocationSrcDst(df, queens, 'qns_src', 'qns_dst')\n",
    "    df = setLocationSrcDst(df, brooklyn, 'bkny_src', 'bkny_dst')\n",
    "    df = setLocationSrcDst(df, bronx, 'bx_src', 'bx_dst')\n",
    "    df = setLocationSrcDst(df, staten_island, 'si_src', 'si_dst')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad features\n",
    "\n",
    "These features shouldn't be used because they:\n",
    "- Don't improve the score;\n",
    "- Are mostly based on `fare_amount`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_peak_hour(df):\n",
    "    df['peak_hour'] = df.hour.apply(lambda x: 1 if ((x >= 7 and x <= 9) or (x >= 16 and x <= 18)) else 0)\n",
    "    df['peak_hour'] = train_df['peak_hour'].astype('bool')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ratio for (rides_per_hour and rides_per_day) (only works for the training set)\n",
    "def add_ride_statistics(df):    \n",
    "    tmp_hour = df.groupby(['year','month','day','hour']).agg({'fare_amount' : [np.size]}).reset_index()\n",
    "    tmp_hour.columns = ['year','month','day','hour','rides_per_hour']\n",
    "\n",
    "    tmp_day = df.groupby(['year','month','day']).agg({'fare_amount' : [np.size]}).reset_index()\n",
    "    tmp_day.columns = ['year','month','day','rides_per_day']\n",
    "            \n",
    "    df = pd.merge(df, tmp_hour, how='left', on=['year','month','day','hour'])\n",
    "    df = pd.merge(df, tmp_day, how='left', on=['year','month','day'])\n",
    "        \n",
    "    df['rides_per_hour'] = df['rides_per_hour'].astype('int16')\n",
    "    df['rides_per_day'] = df['rides_per_day'].astype('int16')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean/median/min/max fare (only works well for the training/test set, not for the Kaggle scoring system)\n",
    "def add_fare_statistics(df):\n",
    "    tmp = df.groupby(['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'fare_amount' : [np.mean,np.median,np.min,np.max]}).reset_index()\n",
    "    tmp.columns = ['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "    df = pd.merge(df, tmp, how='left', on=['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cost per KM (only works well for the training/test set, not for the Kaggle scoring system)\n",
    "def add_cost_per_km(df):\n",
    "    df['cost_per_km'] = df.apply(lambda row: row['distance_km_round'] / row['fare_amount'], axis=1)    \n",
    "    df['cost_per_km'] = df['cost_per_km'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Retrive a clean dataframe from the disk with N rows\n",
    "train_df = get_clean_dataframe(2*1000000) # loads 2M rows\n",
    "\n",
    "### Add relevant features\n",
    "train_df = add_calendar(train_df)\n",
    "train_df = add_distance_km(train_df)\n",
    "train_df = add_distances_mht_ecd(train_df)\n",
    "train_df = add_geo_coords_rounded(train_df)\n",
    "train_df = add_geo_cluster(train_df)\n",
    "train_df = add_geo_diff(train_df)\n",
    "train_df = add_airports(train_df)\n",
    "train_df = add_locations(train_df)\n",
    "\n",
    "# bad features\n",
    "# train_df = add_ride_statistics(train_df)\n",
    "# train_df = add_peak_hour(train_df)\n",
    "# train_df = add_fare_statistics(train_df)\n",
    "# train_df = add_cost_per_km(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subset = train_df[:6000]\n",
    "# sns.pairplot(train_subset, vars=['fare_amount','distance_km_round','lon_diff','lat_diff','geo_cluster','year','month','hour'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression, RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from itertools import compress\n",
    "\n",
    "target = train_df[['fare_amount']]\n",
    "df = train_df.drop(['fare_amount', 'pickup_datetime'], axis=1)\n",
    "\n",
    "X = df\n",
    "y = target.values.ravel()\n",
    "\n",
    "# Split X,y into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('>>> Train/Test Split done!')\n",
    "\n",
    "# Create the model using RandomForestRegressor\n",
    "rfr_model = RandomForestRegressor(n_jobs=7, n_estimators=25, max_features=len(df.columns), max_depth=25, min_samples_split=3, min_samples_leaf=3, random_state=24)\n",
    "rfr_model.fit(X_train, np.log1p(y_train))\n",
    "print('>>> Model Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Hyperparameter Tuning\n",
    "\n",
    "Change `EXEC_GRID_SEARCH` to `True` to run RandomizedSearchCV and find optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "EXEC_GRID_SEARCH = True\n",
    "\n",
    "if (EXEC_GRID_SEARCH):\n",
    "    param_grid = {\n",
    "        'n_estimators': np.linspace(10, 100).astype(int),\n",
    "        'max_depth': [None] + list(np.linspace(5, 30).astype(int)),\n",
    "        'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "        'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "#         'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Estimator for use in random search\n",
    "    estimator = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Create the random search model\n",
    "    rs_cv = RandomizedSearchCV(estimator, param_grid, n_jobs=7, scoring='neg_mean_absolute_error', cv=3, n_iter=100, verbose=10, random_state=24)\n",
    "    rs_cv.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "    rfr_model = rs_cv.best_estimator_\n",
    "    print(f'The best parameters were {rs_cv.best_params_} with a negative mae of {rs_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import gfx  # gfx is another local module under the 'lib' directory\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "# The prediction is calculated as the arithmetic mean of both model's predictions\n",
    "# gbr_fare = np.expm1(gbr_model.predict(X_test))  # model 1\n",
    "rfr_fare = np.expm1(rfr_model.predict(X_test))   # model 2\n",
    "\n",
    "# y_pred = (gbr_visitors + rfr_visitors) / 2\n",
    "# y_pred = gbr_fare\n",
    "y_pred = rfr_fare\n",
    "\n",
    "rmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('\\n>>> RMSE Score:', rmse_score, '\\n') # 2.5M -> 3.6734\n",
    "\n",
    "# Plot Actual fare data against Predicted fare\n",
    "gfx.plot_actual_vs_predicted(y, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[:df.shape[1]]\n",
    "importances = rfr_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission\n",
    "\n",
    "### 5.1 Feature Engineering for the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ride-based features for the test set, copying from the training set\n",
    "def test_add_ride_statistics(test, train):\n",
    "    short_train_df = train[['year','month','day','hour','rides_per_hour']]\n",
    "    tmp_hour = short_train_df.groupby(['year','month','day','hour']).agg({'rides_per_hour' : [np.mean]}).reset_index()\n",
    "    tmp_hour.columns = ['year','month','day','hour','rides_per_hour']\n",
    "    test = pd.merge(test, tmp_hour, how='left', on=['year','month','day','hour']) \n",
    "\n",
    "    short_train_df = train[['year','month','day','rides_per_day']]\n",
    "    tmp_day = short_train_df.groupby(['year','month','day']).agg({'rides_per_day' : [np.mean]}).reset_index()\n",
    "    tmp_day.columns = ['year','month','day','rides_per_day']\n",
    "    test = pd.merge(test, tmp_day, how='left', on=['year','month','day'])\n",
    "    \n",
    "    # At this point, there might be some NaNs laying around    \n",
    "    test[\"rides_per_day\"].fillna(method='ffill', inplace=True) \n",
    "    test[\"rides_per_hour\"].fillna(method='ffill', inplace=True) \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean/median/min/max fare statistics for the test set\n",
    "def test_add_statistics(test, train):\n",
    "    # Score 7.82    \n",
    "#     short_train_df = train[['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "#     tmp = short_train_df.groupby(['year','month','day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "#     tmp.columns = ['year','month','day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']    \n",
    "#     test = pd.merge(test, tmp, how='left', on=['year','month','day_of_week','hour','distance_km_round','taxis_per_hour'])          \n",
    "\n",
    "    # Score 6.59\n",
    "    short_train_df = train[['hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "    tmp = short_train_df.groupby(['hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "    tmp.columns = ['hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "    test = pd.merge(test, tmp, how='left', on=['hour','distance_km_round','taxis_per_hour'])  \n",
    "\n",
    "    # Score 7.58\n",
    "#     short_train_df = train[['day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']]\n",
    "#     tmp = short_train_df.groupby(['day_of_week','hour','distance_km_round','taxis_per_hour']).agg({'mean_fare':[np.mean],'median_fare':[np.mean],'min_fare':[np.mean],'max_fare':[np.mean]}).reset_index()\n",
    "#     tmp.columns = ['day_of_week','hour','distance_km_round','taxis_per_hour','mean_fare','median_fare','min_fare','max_fare']\n",
    "#     test = pd.merge(test, tmp, how='left', on=['day_of_week','hour','distance_km_round','taxis_per_hour'])  \n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_cost_per_km(test):    \n",
    "    test['cost_per_km'] = test.apply(lambda row: row['distance_km_round'] / row['mean_fare'], axis=1)\n",
    "    test['cost_per_km'] = test['cost_per_km'].astype('float32')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load test data and add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads a sample submission file in the correct format (columns key and fare_amount). \n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "# display(submission_df.head(2))\n",
    "\n",
    "# loads the test dataset used for submission\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.drop('passenger_count', axis=1, inplace=True) # remove feature passenger count\n",
    "\n",
    "# display(test_df.head(3))\n",
    "print('>>> test_df shape:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add common features\n",
    "test_df['pickup_datetime'] = test_df['pickup_datetime'].str.slice(0, 16)\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
    "\n",
    "test_df = add_calendar(test_df)\n",
    "test_df = add_distance_km(test_df)\n",
    "test_df = add_distances_mht_ecd(test_df)\n",
    "test_df = add_geo_coords_rounded(test_df)\n",
    "test_df = add_geo_cluster(test_df, train_df)\n",
    "test_df = add_geo_diff(test_df)\n",
    "test_df = add_airports(test_df)\n",
    "test_df = add_locations(test_df)\n",
    "\n",
    "# Bad features\n",
    "# test_df = test_add_ride_statistics(test_df, train_df)\n",
    "# test_df = add_peak_hour(test_df)\n",
    "\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = add_fare_statistics(test_df, train_df)        \n",
    "# display(test_df.head())\n",
    "# print(test_df.isnull().sum())               # check missing data\n",
    "# test_df = test_df.fillna(test_df.mean())    # handle missing data\n",
    "\n",
    "\n",
    "# test_df = test_add_statistics(test_df, train_df)        \n",
    "# display(test_df.head())\n",
    "# print(test_df.isnull().sum())               # check missing data\n",
    "# test_df = test_df.fillna(test_df.mean())    # handle missing data\n",
    "\n",
    "\n",
    "# test_df = test_add_cost_per_km(test_df)\n",
    "# display(test_df.head())\n",
    "\n",
    "# Print NaN rows\n",
    "# print(test_df.isnull().sum())               # check missing data\n",
    "display(test_df[test_df.isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "L1 = train_df.columns.tolist()\n",
    "L2 = test_df.columns.tolist()\n",
    "print('>>> Only *fare_amount* can show up as the difference between both datasets.\\n>>> Found:', list(set(L1) - set(L2)))\n",
    "print('>>> Number of Features for training=', len(L1), 'and testing=', len(L2))\n",
    "\n",
    "tc = unittest.TestCase('__init__')\n",
    "tc.assertEqual(len(L1) == len(L2), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create (full) model and predict the fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Prepare the data\n",
    "X = train_df.drop(['fare_amount', 'pickup_datetime'], axis=1)\n",
    "y = train_df[['fare_amount']].values.ravel()\n",
    "\n",
    "### Create model\n",
    "rfr_model = RandomForestRegressor(n_jobs=7, n_estimators=25, max_features=len(df.columns), max_depth=25, min_samples_split=3, min_samples_leaf=3, random_state=42)\n",
    "# rfr_model = RandomForestRegressor(n_jobs=6, bootstrap=True, n_estimators=41, max_features=0.5, max_depth=22, min_samples_split=2, min_samples_leaf=3, max_leaf_nodes=49, random_state=42)\n",
    "rfr_model.fit(X, np.log1p(y))\n",
    "\n",
    "### Predict fare\n",
    "X_test = test_df.drop(['key', 'pickup_datetime'], axis=1)\n",
    "y_pred = np.expm1(rfr_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save submission file\n",
    "submission_df['fare_amount'] = y_pred\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "display(submission_df.head())\n",
    "print('>>> submission_df shape=', submission_df.shape)\n",
    "print('>>> File saved sucessfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print total notebook runtime for debugging purposes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "elapsed_time = (time.time() - start_time)\n",
    "print('>>> Runtime:', str(timedelta(seconds=elapsed_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
